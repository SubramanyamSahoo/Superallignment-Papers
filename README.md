# Superallignment-Papers

## The goal is to build safe AI which must be implicitly responsible 


## Here I will markdown all papers related to superallignment 



### 1.WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION (https://arxiv.org/abs/2312.09390) OpenAI
### 2.Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training (https://arxiv.org/abs/2401.05566) Anthropic
### 3.A Hazard Analysis Framework for Code Synthesis Large Language Models (https://arxiv.org/abs/2207.14157) OpenAI
### 4.Frontier AI Regulation: Managing Emerging Risks to Public Safety (https://arxiv.org/abs/2307.03718) OpenAI
### 5.Building an early warning system for LLM-aided biological threat creation (https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation#ref-B) OpenAI


