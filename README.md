# Superallignment-Papers

## The goal is to build safe AI which must be implicitly responsible 


## Here I will markdown all papers related to superallignment 



### 1.WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION (https://arxiv.org/abs/2312.09390) OpenAI
### 2.Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training (https://arxiv.org/abs/2401.05566) Anthropic


